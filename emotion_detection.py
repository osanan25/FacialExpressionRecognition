# -*- coding: utf-8 -*-
"""Emotion_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dpfp4e1J4V4yZxHPTRvspYks3GS38fjg

#***Emotion Detection Final NVIDIA Project - Om Sanan***

---
"""

from google.colab import drive
drive.mount('/content/drive')

import math
import numpy as np
import pandas as pd

import scikitplot
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, Activation
from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from keras.utils import np_utils
from PIL import Image, ImageDraw

df = pd.read_csv('/content/fer2013.csv')
print(df.shape)
df.head()

df.emotion.unique() #Emotions labeled 0-6

emotion_text = {0:'anger', 1:'disgust', 2:'fear', 3:'happy', 4: 'sad', 5: 'surprise', 6: 'neutral'}

df.emotion.value_counts()

#checking images in each category
sns.countplot(df.emotion)
plt.show()

#display 8 images per emotion
fig = plt.figure(1, (14, 14))

k = 0
for label in sorted(df.emotion.unique()):
    for i in range(8):
        px = df[df.emotion==label].pixels.iloc[k]
        px = np.array(px.split(' ')).reshape(48, 48).astype('float32')

        k += 1
        ax = plt.subplot(8, 8, k)
        ax.imshow(px, cmap='gray')
        ax.set_xticks([])
        ax.set_yticks([])
        ax.set_title(emotion_text[label])
        plt.tight_layout()

#Start Here
categories = [0, 1, 2, 3, 4, 5, 6]
df = df[df.emotion.isin(categories)]
df.shape

#making data nicer
img_array = df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48, 48, 1).astype('float32'))
img_array = np.stack(img_array, axis=0)
img_array.shape

le = LabelEncoder()
img_labels = le.fit_transform(df.emotion)
img_labels = np_utils.to_categorical(img_labels)
img_labels.shape

le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(le_name_mapping)
#Start Here

#splitting our data with test train split for out model
X_Train, X_Test, Y_Train, Y_Test = train_test_split(img_array, img_labels,
                                                    shuffle=True, stratify=img_labels,
                                                    test_size=0.1, random_state=42)
X_Train.shape, X_Test.shape, Y_Train.shape, Y_Test.shape

img_width = X_Train.shape[1]
img_height = X_Train.shape[2]
img_depth = X_Train.shape[3]
num_classes = Y_Train.shape[1]

#Normalizing data on a scale 0 to 1
X_train = X_Train / 255.0
X_valid = X_Test / 255.0

def build_net(optim):
#DCNN Network
    net = Sequential(name='DCNN')

    net.add(
        Conv2D(
            filters=64,
            kernel_size=(5,5),
            input_shape=(img_width, img_height, img_depth),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_1'
        )
    )
    net.add(BatchNormalization(name='batchnorm_1'))
    net.add(
        Conv2D(
            filters=64,
            kernel_size=(5,5),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_2'
        )
    )
    net.add(BatchNormalization(name='batchnorm_2'))
    
    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_1'))
    net.add(Dropout(0.4, name='dropout_1'))

    net.add(
        Conv2D(
            filters=128,
            kernel_size=(3,3),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_3'
        )
    )
    net.add(BatchNormalization(name='batchnorm_3'))
    net.add(
        Conv2D(
            filters=128,
            kernel_size=(3,3),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_4'
        )
    )
    net.add(BatchNormalization(name='batchnorm_4'))
    
    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_2'))
    net.add(Dropout(0.4, name='dropout_2'))

    net.add(
        Conv2D(
            filters=256,
            kernel_size=(3,3),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_5'
        )
    )
    net.add(BatchNormalization(name='batchnorm_5'))
    net.add(
        Conv2D(
            filters=256,
            kernel_size=(3,3),
            activation='elu',
            padding='same',
            kernel_initializer='he_normal',
            name='conv2d_6'
        )
    )
    net.add(BatchNormalization(name='batchnorm_6'))
    
    net.add(MaxPooling2D(pool_size=(2,2), name='maxpool2d_3'))
    net.add(Dropout(0.5, name='dropout_3'))

    net.add(Flatten(name='flatten'))
        
    net.add(
        Dense(
            128,
            activation='elu',
            kernel_initializer='he_normal',
            name='dense_1'
        )
    )
    net.add(BatchNormalization(name='batchnorm_7'))
    
    net.add(Dropout(0.6, name='dropout_4'))
    
    net.add(
        Dense(
            num_classes,
            activation='softmax',
            name='out_layer'
        )
    )
    
    net.compile(
        loss='categorical_crossentropy',
        optimizer=optim,
        metrics=['accuracy']
    )
    
    net.summary()
    
    return net

#In my old code, I experienced a lot of issues that were a result of overfitting
#and used this code to help prevent my model from overfitting

early_stopping = EarlyStopping(
    monitor='val_accuracy',
    min_delta=0.00005,
    patience=11,
    verbose=1,
    restore_best_weights=True,
)

lr_scheduler = ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,
    patience=7,
    min_lr=1e-7,
    verbose=1,
)

callbacks = [
    early_stopping,
    lr_scheduler,
]

train_datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.15,
    height_shift_range=0.15,
    shear_range=0.15,
    zoom_range=0.15,
    horizontal_flip=True,
)
train_datagen.fit(X_Train)

optims = [
    optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam'),
    optimizers.Adam(0.001),
]

# I tried both `Nadam` and `Adam`, the difference in results is not different but I finally went with Nadam as it is more popular.
model = build_net(optims[1]) 
history = model.fit_generator(
    train_datagen.flow(X_Train, Y_Train, batch_size=32), #batch size of 32 performs the best
    validation_data=(X_Test, Y_Test),
    steps_per_epoch=len(X_train) / 32,
    epochs=25,
    callbacks=callbacks,
    use_multiprocessing=True
)

model_yaml = model.to_json()
with open("model.yaml", "w") as yaml_file:
    yaml_file.write(model_yaml)
    
model.save("fModel.h5")

model.save("fModel.h5")

model = tf.keras.models.load_model("/content/fModel.h5")

df_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})
df_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})

fig = plt.figure(0, (14, 4))
ax = plt.subplot(1, 2, 1)
sns.violinplot(x="variable", y="value", data=pd.melt(df_accu), showfliers=False)
plt.title('Accuracy')
plt.tight_layout()

ax = plt.subplot(1, 2, 2)
sns.violinplot(x="variable", y="value", data=pd.melt(df_loss), showfliers=False)
plt.title('Loss')
plt.tight_layout()

plt.savefig('performance_dist.png')
plt.show()

#There was good training along the way, but the model started overfitting at the end.

cats = {
    0: "happy",
    1: "sad",
    2: "neutral",
}

# import modules
from IPython.display import display, Javascript, Image
from google.colab.output import eval_js
from base64 import b64decode, b64encode
import cv2
import numpy as np
import PIL
import io
import html
import time

# function to convert the JavaScript object into an OpenCV image
def imgFunc(js_reply):
  # decode base64 image
  image_bytes = b64decode(js_reply.split(',')[1])
  # convert bytes to numpy array
  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)
  # decode numpy array into OpenCV BGR image
  img = cv2.imdecode(jpg_as_np, flags=1)

  return img

# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream
def pic_to_bytes(pic_array):
  # convert array into PIL image
  pic_PIL = PIL.Image.fromarray(pic_array, 'RGBA')
  iobuf = io.BytesIO()
  # format pic into png for return
  pic_PIL.save(iobuf, format='png')
  # format return string
  pic_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))

  return pic_bytes

# initialize the Haar Cascade face detection model
face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))

def take_photo(filename='photo.jpg', quality=0.8):
  js = Javascript('''
    async function takePhoto(quality) {
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({video: true});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize the output to fit the video element.
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      // Wait for Capture to be clicked.
      await new Promise((resolve) => capture.onclick = resolve);

      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getVideoTracks()[0].stop();
      div.remove();
      return canvas.toDataURL('image/jpeg', quality);
    }
    ''')
  display(js)

  # get photo data
  data = eval_js('takePhoto({})'.format(quality))
  # get OpenCV format image
  img = imgFunc(data) 
  # grayscale img
  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
  print(gray.shape)
  # get face bounding box coordinates using Haar Cascade
  faces = face_cascade.detectMultiScale(gray)
  # draw face bounding box on image
  for (x,y,w,h) in faces:
      img = cv2.rectangle(img, (x,y),(x+w,y+h),(255,0,0),2)
      crop_img = img[y:y+h, x:x+w]
      cv2.waitKey(0)

  # save image
  cv2.imwrite(filename, crop_img)

  return filename

#crops face
try:
  filename = take_photo('photo.jpg')
  print('Saved to {}'.format(filename))
  # Show the image which was just taken.
  display(Image.open(filename))
except Exception as err:
  # Error if webcam is not
  print(str(err))

#if this doesn't work, rerun the cell

path = "/content/drive/MyDrive/Training/0/35010.png"

path = "/content/drive/MyDrive/Training/3/1006.png"

path = "/content/drive/MyDrive/Training/5/4444.png"

path = "/content/photo.jpg"

def prepare(path):
  img_array = cv2.imread(path)
  new_array = cv2.resize(img_array, (48, 48))
  new_array = new_array.reshape(-1, 48, 48, 1)
  new_array = new_array/255
  return new_array

predictions = model.predict(prepare(path))

#predictions[0]

tempPre = str(predictions[0])
tempPre2 = tempPre.replace("[", "")
tempPre3 = tempPre2.replace("]", "")

pree = tempPre3.split()
fPree = float(pree[0])
fPree2 = float(pree[np.argmax(pree)])

pre = np.argsort(predictions[0])
#print(pre)

casePred = str(pre[0])
tempPred = str(pre[5])
tempPred2 = tempPred.replace("[[", "")
pred = tempPred2.replace("]", "")
pred
intPred = int(pred)

if(fPree > fPree2):
  print("The person is angry!")
  i = 1

if int(str(pre[6])) == 0:
  i = 0
  if ((intPred) == 1):
    print("The person is disgusted!")
  if ((intPred) == 2):
    print("The person is fearful!")
  if ((intPred) == 3):
    print("The person is happy!")
  if ((intPred) == 4):
    print("The person is sad!")
  if ((intPred) == 5):
    print("The person is suprised!")
  if ((intPred) == 6):
    print("The person is neutral!")

img = cv2.imread(path)
img = np.float32(img)

if (i == 1):
  if (fPree == fPree2):
      status = "Anger" 
      imgg = Image.open(path)
      I1 = ImageDraw.Draw(imgg)
      I1.text((20, 30), status)
      imgg.save("finalPrediction.png")
      image = "finalPrediction.png" 
else:
  if (intPred == 1):
      status = "Disgust" 
      imgg = Image.open(path)
      I1 = ImageDraw.Draw(imgg)
      I1.text((10, 10), status, fill=(255, 0, 0))
      imgg.save("finalPrediction.png")
      image = "finalPrediction.png"  

  if (intPred == 2):
      status = "Fear" 
      imgg = Image.open(path)
      I1 = ImageDraw.Draw(imgg)
      I1.text((10, 10), status, fill=(255, 0, 0))
      imgg.save("finalPrediction.png")
      image = "finalPrediction.png" 

  if (intPred == 3):
      status = "Happy" 
      imgg = Image.open(path)
      I1 = ImageDraw.Draw(imgg)
      I1.text((15, 37), status)
      imgg.save("finalPrediction.png")
      image = "finalPrediction.png"  

  if (intPred == 4):
      status = "Sad - Sign of Depression" 
      imgg = Image.open(path)
      I1 = ImageDraw.Draw(imgg)
      I1.text((10, 10), status, fill=(255, 0, 0))
      imgg.save("finalPrediction.png")
      image = "finalPrediction.png" 

  if (intPred == 6):
      status = "Suprise" 
      imgg = Image.open(path)
      I1 = ImageDraw.Draw(imgg)
      I1.text((10, 10), status, fill=(255, 0, 0))
      imgg.save("finalPrediction.png")
      image = "finalPrediction.png" 

  if (intPred == 6):
      status = "Neutral" 
      imgg = Image.open(path)
      I1 = ImageDraw.Draw(imgg)
      I1.text((10, 13), status, fill=(255, 0, 0))
      imgg.save("finalPrediction.png")
      image = "finalPrediction.png"

final_image = cv2.imread(image)
plt.imshow(cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB))
plt.grid(None)